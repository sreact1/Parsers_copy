{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "print artm.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаю батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='',\n",
    "                                        data_format='bow_uci',\n",
    "                                        collection_name='wikistem',\n",
    "                                        target_folder='wikistem_batches',                                 \n",
    "                                        batch_size=1000)\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='',\n",
    "                                        data_format='bow_uci',\n",
    "                                        collection_name='wikilemm',\n",
    "                                        target_folder='wikilemm_batches',\n",
    "                                        batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаю на основе батчей словари и сохраняю их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_stem = artm.Dictionary()\n",
    "dictionary_stem.gather(data_path='wikistem_batches')\n",
    "dictionary_stem.save_text(dictionary_path='wikistem_batches/dictionary_stem.txt')\n",
    "\n",
    "dictionary_lemm = artm.Dictionary()\n",
    "dictionary_lemm.gather(data_path='wikilemm_batches')\n",
    "dictionary_lemm.save_text(dictionary_path='wikilemm_batches/dictionary_lemm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузка уже существующих батчей и словарей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer_stemm = artm.BatchVectorizer(data_path='wikistem_batches',\n",
    "                                        data_format='batches')\n",
    "\n",
    "dictionary_stem = artm.Dictionary()\n",
    "dictionary_stem.load_text(dictionary_path='wikistem_batches/dictionary_stem.txt')\n",
    "\n",
    "batch_vectorizer_lemm = artm.BatchVectorizer(data_path='wikilemm_batches',\n",
    "                                        data_format='batches')\n",
    "\n",
    "dictionary_lemm = artm.Dictionary()\n",
    "dictionary_lemm.load_text(dictionary_path='wikilemm_batches/dictionary_lemm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценивание LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим саму тематическую модель, указав число тем и параметр числа прохода по документу, гиперпараметры сглаживания матриц Φ и Θ, а также используемый словарь. Кроме того, попросим сохранять матрицу Θ, чтобы потом на неё можно было взглянуть.\n",
    "\n",
    "Здесь же можно указать параметр num_processors, отвечающий за число потоков, которые будут использованы для вычислений на машине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = artm.LDA(num_topics=400, alpha=0.01, beta=0.001,\n",
    "               num_document_passes=10, dictionary=dictionary_lemm,\n",
    "               cache_theta=True,num_processors=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.fit_offline(batch_vectorizer=batch_vectorizer_lemm, num_collection_passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на значения перпелексии на каждой итерации прохода по коллекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.perplexity_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, можно посмотреть на наиболее вероятные слова в каждой теме. Они выдаются в виде списка списков строк (каждый внутренний список соответствуюет одной теме, по порядку). Выведем их, предварительно отформатировав:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_tokens = lda.get_top_tokens(num_tokens=30)\n",
    "for i, token_list in enumerate(top_tokens):\n",
    "    print 'Topic #{0}: {1}'.format(i, token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно получить доступ к матрицам с помощью следующих вызовов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi = lda.phi_\n",
    "theta = lda.get_theta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем ещё ряд возможностей модели artm.LDA.\n",
    "\n",
    "Во-первых - это построение матрицы Θ для новых документов при наличии обученной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='kos_batches_test')\n",
    "theta_test = lda.transform(batch_vectorizer=test_batch_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, в том случае, если требуется производить различную регуляризацию каждой из тем в матрице ΦΦ, вместо скалярного значения beta можно задать список гиперпараметров, длиной в число тем, и каждая тема будет регуляризована со своим гиперпараметром:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = [0.1] * num_topics  # change as you need\n",
    "lda = artm.LDA(num_topics=15, alpha=0.01, beta=beta, num_document_passes=5, dictionary=dictionary, cache_theta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем нужную нам информацию. Вектор из пералексий, самые популярные слова и саму модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Сохраняем вектор из перплексий\n",
    "a=lda.perplexity_value\n",
    "with open('lda_lemm_400_perp.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle)\n",
    "\n",
    "# Сохраняем топики из слов\n",
    "top_tokens = lda.get_top_tokens(num_tokens=30)\n",
    "with open('lda_lemm_400_top_tokens.pickle', 'wb') as handle:\n",
    "    pickle.dump(top_tokens, handle)    \n",
    "    \n",
    "# Сохраняем модель\n",
    "lda.save(\"lda_400_lemm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PLSA обычная и с небольшой частичкой волшебства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer_lemm = artm.BatchVectorizer(data_path='wikilemm_batches',\n",
    "                                        data_format='batches')\n",
    "\n",
    "dictionary_lemm = artm.Dictionary()\n",
    "dictionary_lemm.load_text(dictionary_path='wikilemm_batches/dictionary_lemm.txt')\n",
    "\n",
    "T = 400\n",
    "my_dictionary = dictionary_lemm\n",
    "batch_vectorizer = batch_vectorizer_lemm\n",
    "\n",
    "# создаём модель\n",
    "model = artm.ARTM(num_topics=T, dictionary=my_dictionary, cache_theta=False)\n",
    "# Когда в коллекции много документов, матрицу лучше не запоминать!\n",
    "\n",
    "# вешаем на неё перплексию\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity_score',\n",
    "                                      dictionary=my_dictionary))\n",
    "\n",
    "# Вешаем метрики разряженности матриц\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))\n",
    "model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))\n",
    "\n",
    "# Вешаем запоминатель самых вероятных слов, будем запоминать 30 штук\n",
    "model.scores.add(artm.TopTokensScore(name='top_tokens_score', num_tokens=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Оффлайновый алгоритм: много проходов по коллекции, один проход по документу (опционально), обновление Φ в конце каждого прохода. Используйте, если у Вас маленькая коллекция.\n",
    "\n",
    "* Онлайновый алгоритм: один проход по коллекции (опционально), много проходов по документу, обновление Φ раз в заданное количество батчей. Используйте при большой коллекции, и коллекции с быстро меняющейся тематикой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.num_document_passes = 10  # сколько делать проходов по каждому документу\n",
    "\n",
    "import time\n",
    "start_time = time.time() # время работы кода \n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=20)\n",
    "print \"--- %s seconds ---\" % (time.time() - start_time)  #время работы кода\n",
    "#Юпи!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# распечатаем все значения перплесии и других метрик\n",
    "print model.score_tracker['perplexity_score'].value      # .last_value\n",
    "print model.score_tracker['sparsity_phi_score'].value    # .last_value\n",
    "print model.score_tracker['sparsity_theta_score'].value  # .last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ежели нас что-то не устраивает, то дообучаем модель делая еще например 15 итераций.\n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Да, последний момент: метрики выгружаются из ядра при каждом обращении, поэтому для таких больших метрик, как топ-слова (или ядровые характеристики, о которых можно прочитать по данным выше ссылкам), лучше завести переменную, в которую Вы один раз всё выгрузите, а потом уже работать с ней. Поехали, просмотрим топ-слова последовательно в цикле по именам тем модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_top_tokens = model.score_tracker['top_tokens_score'].last_tokens\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "    print saved_top_tokens[topic_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Наверняка темы получились не слишком впечатляющими, да? И вот сейчас на сцену выходят они, озарённые сиянием прожекторов звёзды Тематической империи - регуляризаторы! Их задача - помочь Вам сделать модель как можно более хорошей и милой.\n",
    "\n",
    "Списки регуляризаторов и их параметров можно посмотреть здесь http://bigartm.readthedocs.org/en/master/python_interface/regularizers.html. Код работы с регуляризаторами очень похож на код работы с метриками (они всё-таки дальние родственники). Давайте в нашу модель добавим три регуляризатора: разреживание Φ, разреживание Θ и декорреляция тем. Последний в поте лица старается сделать темы как можно более различными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='sparse_phi_regularizer'))\n",
    "model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='sparse_theta_regularizer'))\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, у Вас вызывает когнитивный диссонанс регуляризатор SmoothSparsePhi\\Theta, это он что же, и сглаживает, и разреживает? И мой ответ - именно так. Только не \"и\", а \"или\" (хотя для ΦΦ можно и \"и\", об этом дальше будет). Он может и то, и то, его действия будут зависеть от того, каким Вы зададите его коэффициент регуляризации ττ (если не знаете, что это - обращайтесь к работам архимага). ττ > 0 - будет сглаживать, ττ < 0 - разреживать. По-умолчанию все регуляризаторы получают ττ = 1.0, что, как правило, совершенно не подходит. Выбор подходящего ττ - эвристика, искусство заклинателя, порой приходится провести десятки опытов, чтобы подобрать хорошие значений. Я не буду здесь этим заниматься, просто покажу, как выставлять эти значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.regularizers['sparse_phi_regularizer'].tau = -5*1e5\n",
    "model.regularizers['sparse_theta_regularizer'].tau = -1000\n",
    "model.regularizers['decorrelator_phi_regularizer'].tau = 1e+5\n",
    "\n",
    "# В темах много общеупотребительных слов (так называемой, фоновой лексики). \n",
    "# Чтобы этого избежать, будем использовать разреживающий регуляризатор для матрицы фи.\n",
    "# Он будет подавлять слова, которые имеют большую частоту во всей коллекции.\n",
    "\n",
    "# Меняем так, чтобы добиться разряжености - более высокие значения метрик (частота нулей в матрице) \n",
    "# и интерпретируемости тем. Лучше всего применять его после того как модель сошлась чтобы добиться интерпретации!\n",
    "# То есть сначала обучили модель, после ввели метрики, после дообучили\n",
    "\n",
    "model.regularizers['sparse_phi_regularizer'].tau = -100\n",
    "model.regularizers['sparse_phi_regularizer'].tau = -5*1e4 \n",
    "\n",
    "# если вы хотите применять регуляризатор только к некоторым модальностям,\n",
    "# указывайте это в параметре class_ids: class_ids=[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выставленные значения стандартны, но при неблагоприятном стечении обстоятельств могут либо не оказать на модель существенного влияния, либо заставить её корчиться в муках. Берегите свои модели, подбирайте коэффициенты тщательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# дообучаем модель\n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше можно снова смотреть на метрики, поправлять коэффиценты ττ регуляризаторов и т.д.\n",
    "\n",
    "Забыл упомянуть, что регуляризаторам, как и метрикам, тоже можно объяснить, какие темы можно трогать, а какие - нельзя (они всё-таки неглупые ребята). Делается это в полной аналогии с тем, как мы проделали это для метрик. Не буду приводить пример из принципа, сами догадайтесь, это элементарно. (http://nbviewer.jupyter.org/github/bigartm/bigartm-book/blob/master/ARTM_tutorial_Fun.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Сохраняем топики из слов\n",
    "top_tokens = model.score_tracker['top_tokens_score'].last_tokens\n",
    "with open('plsa_lemm_200_top_tokens.pickle', 'wb') as handle:\n",
    "    pickle.dump(top_tokens, handle)   \n",
    "\n",
    "# Сохраняем метрики\n",
    "a1 = model.score_tracker['perplexity_score'].value      # .last_value\n",
    "a2 = model.score_tracker['sparsity_phi_score'].value    # .last_value\n",
    "a3 = model.score_tracker['sparsity_theta_score'].value  # .last_value\n",
    "\n",
    "a = {'perep':a1,'phi':a2,'theta':a3}\n",
    "with open('plsa_lemm_200_scores.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
